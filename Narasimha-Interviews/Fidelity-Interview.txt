Q1: What are your technical skills and expertise?

A1:
I have good knowledge of Java, specifically with the JDK 17 version, along with expertise in Spring and Spring Boot frameworks. I have a strong understanding of microservices architecture. For databases, I am skilled in MySQL and MongoDB. On the front-end side, I am well-versed in HTML, CSS, JavaScript, and Angular (17 version).

Q2: Could you tell us about your experience working at Amazon Robotics?

A2:
At Amazon Robotics, I worked as a Senior Principal Architect, primarily focusing on the delivery team. My work involved handling delivery applications for e-commerce. The team followed an Agile methodology with around 10 members, and I led the team in ensuring the smooth delivery of the product. The system was event-driven with microservices architecture, utilizing Spring Boot and Kafka for messaging.

Q3: What AWS services have you worked with in your projects?

A3:
I have worked with various AWS services like EC2, ECS, ECR, and EKS. For serverless architecture, I have experience with AWS Lambda, along with notification services such as SNS and SQS.

Q4: How does the architecture of Amazon Robotics work in terms of microservices?

A4:
Amazon Robotics follows an event-driven microservices architecture where various services communicate using Kafka for messaging. For example, when a customer places an order, the order service sends notifications to the vendor, which is then communicated to the logistics department for order segregation and assignment. The entire process ensures efficient product delivery. The services are developed using AWS Lambda, and the database used is DynamoDB.

Q5: Can you elaborate on your experience with micro frontends?

A5:
I have worked on a micro front-end application where different teams are responsible for specific features or components. This approach allows parallel development, helping teams independently develop, test, and deploy their parts of the front-end. The architecture includes separate modules for robot status, warehouse mapping, inventory, and more, managed by different teams.

Q6: How does the integration between front-end and back-end work in Amazon Robotics?

A6:
For the front-end, we used React JS, and the back-end APIs are connected using an API framework. We also used reverse proxy servers like Nginx to connect the front-end to the back-end applications. Auto-scaling groups were set up to handle incoming requests from the UI while ensuring scalability.

Q7: How do you handle file uploads in your application?

A7:
For file uploads, we create a file input component in the front-end using React. The file validation ensures that only specific file types (e.g., Excel files) are uploaded. The uploaded file data is sent to the back-end via HTTP POST requests using Axios. On the back-end, we process the file, validate its content, and store it in a database using JPA. For large files, we implement streaming approaches to handle them efficiently without loading everything into memory at once.

Q8: How do you process large Excel files?

A8:
For large Excel files, we ensure efficiency by using streaming techniques to process the file without loading it entirely into memory. If the upload takes longer, we implement server-sent events to provide real-time progress updates to the user. AWS Lambda is triggered when a file is uploaded to an S3 bucket, and it starts processing the file immediately. We also implement retry mechanisms and chunk uploads to handle network interruptions effectively.

Q9: Can you explain the process of handling PDF uploads in your application?

A9:
For PDF uploads, we restrict the file selection to PDFs by using file validation in the front-end. The uploaded PDF file is sent to the back-end, where we use libraries like Apache PDFBox to process and manipulate the PDF content if needed. We can extract text, images, or metadata, or simply store the file without processing. If processing is required, itâ€™s handled in the service layer before saving the file to the database.

Q10: How do you handle network interruptions during file uploads?

A10:
We implement retry mechanisms using libraries like Axios or custom logic to handle file upload retries in case of network failures. Additionally, we use chunked uploads, where the file is divided into smaller parts, and each part is uploaded separately. If a part fails, only that specific chunk is retried. For back-end retry handling, we use Spring's retry mechanism or resilience libraries to ensure smooth operations.

Q11: How do you ensure database synchronization in desktop applications for large datasets?

A11:
To ensure that desktop applications are synchronized with large data sets in real-time, we can use push notifications via message brokers like Kafka. The desktop applications subscribe to these notifications and update their data accordingly. Alternatively, we can use polling, where applications periodically check the server for updates. While the push method ensures real-time synchronization, polling is simpler but might introduce a delay between updates.

Q12: What libraries or tools have you used for test-driven development (TDD)?

A12:
For test-driven development, I have used tools like JUnit for unit testing and Mockito for mocking dependencies. The TDD approach ensures that the code is tested before development, helping to ensure high-quality and reliable code.

Q13: How do you handle file validation during uploads?

A13:
During file uploads, we validate the file type (e.g., Excel or PDF) both on the client and server sides. The back-end uses annotations like @RequestParam to handle the uploaded file and process it with libraries such as Apache POI for Excel files or Apache PDFBox for PDFs. The service layer is responsible for handling any validation or processing before saving the data to the database.

Q14: Can you explain the usage of AWS Lambda in your project?

A14:
AWS Lambda is used to handle serverless computing tasks, such as file processing or event handling. For example, when a file is uploaded to an S3 bucket, a Lambda function is triggered to process the file. It helps automate tasks like data validation, processing, and storing files without the need for dedicated servers.

Q15: How do you ensure efficient handling of large datasets during file uploads?

A15:
For large datasets, I use a streaming approach to process the file without loading it entirely into memory. We also use a pre-signed URL to allow clients to upload files directly to S3. In case of network interruptions, I implement retry mechanisms and chunk uploads to ensure that the file upload is completed successfully.
